{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this jupiter notebook rassemble my work in order\n",
    "#first we import every libraries that we need\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#then we create the file were the datas are gonna go\n",
    "test = f\"./fulldatas.json\"\n",
    "#we initiate the driver for the selenium part\n",
    "def driver_gen():\n",
    "    driver = webdriver.Chrome(executable_path=\"driver/chromedriver\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get data from the list of links func \n",
    "def clean_data(listed):\n",
    "    house_number = 1\n",
    "    dict_of_informations= {}\n",
    "    list_of_link = listed\n",
    "    for lk in list_of_link : \n",
    "        r = requests.get(lk)\n",
    "        content = r.content\n",
    "        property_details_page = BeautifulSoup(content,\"html.parser\")\n",
    "        dict_of_data= {}\n",
    "        #we clean the datas by finding tags\n",
    "        for tr in property_details_page.find_all('tr', attrs={'class':'classified-table__row'}):\n",
    "            #these are the tage were our infos are\n",
    "            value = tr.find('th')\n",
    "            data = tr.find('td')\n",
    "            if value != None :\n",
    "                #we clean them using regex yay !\n",
    "                title = re.sub(r'\\s*<[^>]*>\\s*','',str(value))\n",
    "                if data != None : \n",
    "                    datas = re.sub(r'\\s*<[^>]*>\\s*','',str(data))\n",
    "                    # we delete the big space to replace them with 1 only\n",
    "                    datass = \" \".join(datas.split())\n",
    "                    #we store data in the dict\n",
    "                    dict_of_data[title] = datass\n",
    "                else: \n",
    "                    #if there are no datas we write none\n",
    "                    dict_of_data[title] = None\n",
    "            else: \n",
    "                continue\n",
    "        #we put the datas in a another dict with all the house listed\n",
    "        dict_of_informations[house_number] = dict_of_data\n",
    "        house_number += 1 \n",
    "    return dict_of_informations\n",
    "    #right now the data is are dictionaries in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we list all the links to scrape them after\n",
    "def get_links_and_clean(iterations, driver):\n",
    "    list_of_link = []\n",
    "    #still doing by page number is gonna change\n",
    "    for num in iterations:\n",
    "        page_num = str(num) + \"&orderBy=relevance\"\n",
    "        url = (\n",
    "                \"https://www.immoweb.be/en/search/house-and-apartment/for-sale?countries=BE&page=\" + page_num\n",
    "                )\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        dict_of_informations = {}\n",
    "        for link in soup.find_all(\"a\", class_=\"card__title-link\") : \n",
    "            list_of_link.append(link['href'])\n",
    "    driver.quit()\n",
    "    #calling clean function to clean and return clean data\n",
    "    #not the good way i think incorporate links finding in data cleaning\n",
    "    cleaned = clean_data(list_of_link)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data in a json file\n",
    "def saving_datas(datas):\n",
    "    with open(test, 'a') as file:\n",
    "        json.dump(str(datas), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating of the thread pool\n",
    "#need to return on this function to divide the selenium and requests tasks\n",
    "# map 300 links to the 36*300 houses to the 8 or 16 drivers and threads\n",
    "def pool():\n",
    "    # 1 driver by thread (we have four thread here)\n",
    "    drivers = [driver_gen() for _ in range(8)]\n",
    "    #division of the number of pages by the number of threads \n",
    "    division = np.array_split(np.arange(1,16),8)\n",
    "    #creation of a pool of threads\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor :\n",
    "        #it is where it needs refinement\n",
    "        results = executor.map(get_links_and_clean,division,drivers) \n",
    "        for result in results:\n",
    "            saving_datas(result)\n",
    "            print(result)\n",
    "\n",
    "pool()\n",
    "\n",
    "\n",
    "#bug = i get the same house over and over in my final dictionary \n",
    "#but all the links are different ... Why ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "456673b175213a59691ddae7563f6c552706ad8803eaabf7fe5cd8937a9be15f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
